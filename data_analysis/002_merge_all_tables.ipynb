{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all the datasets into one table\n",
    "\n",
    "The code below processes multiple CSV files, applies anisotropy corrections, calculates clone types, and merges the data into a single DataFrame. \n",
    "\n",
    "It also calculates major and minor axes, elongation, and other metrics, and saves the combined data to a CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.2.1\n",
      "Numpy version: 1.23.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:00<00:01,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1066\n",
      "Processing 1067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:00<00:00,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1069\n",
      "Processing 1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:00<00:00,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1381\n",
      "Processing 883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [00:01<00:00,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 934\n",
      "Processing 935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  5.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np \n",
    "import glob \n",
    "from tqdm import tqdm\n",
    "\n",
    "#print versions of libraries\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "\n",
    "\n",
    "local_pc_path = \"./merged/\"#\"./merged/\"\n",
    "\n",
    "#get list of files with tab extension using glob \n",
    "files = glob.glob(local_pc_path + \"*.csv\")\n",
    "\n",
    "#Create a directory to save final_graphs\n",
    "if not os.path.exists(\"./final_graphs\"):\n",
    "    os.makedirs(\"./final_graphs\")\n",
    "\n",
    "def apply_anisotropy(df,all_px_size,fname,column_name):\n",
    "    #As the images are downsampled in y and x by 3, the pixel sizes need to be multiplied by 3\n",
    "    fname = os.path.basename(file).split('.')[0].split(\"_\")[0]\n",
    "    orig_pixel_sizes = all_px_size[fname]\n",
    "    #image used is from series 2 of czi which is downsampled in z,y,x by 1,3,3\n",
    "    downsample_series2 = (1,3,3)\n",
    "    pixel_sizes = tuple([orig_pixel_sizes[i]*downsample_series2[i] for i in range(3)])\n",
    "    #calculate anisotropy in the downsampled image and apply that to centroid_z column\n",
    "    anisotropy = pixel_sizes[0]/pixel_sizes[2]\n",
    "    df[column_name] = df[column_name]*anisotropy\n",
    "    return df\n",
    "\n",
    "def calc_clone_type(row, clone_proportion_thresh=0):\n",
    "    #Return clone type as a string based on the clone proportions\n",
    "    clone_cols={'1': '1', '10': '2', '11': '3', '100': '4', '101': '5', '110': '6', '111': '7'}\n",
    "    col_iterator = list(clone_cols.keys())  \n",
    "    return \"_\".join([clone_cols[col] for col in col_iterator if row[col]>clone_proportion_thresh])\n",
    "\n",
    "#Metadata for the different files\n",
    "#pixel sizes of full resolution images\n",
    "all_px_size = {\n",
    "    '883': (2.8846, 1.1434, 1.1434),\n",
    "    '934': (2.9169, 1.2196, 1.2196),\n",
    "    '935': (2.8845, 1.2196, 1.2196),\n",
    "    '1066': (2.9169, 1.2196, 1.2196),\n",
    "    '1064': (2.9169, 1.2196, 1.2196),\n",
    "    '1067':(2.9169, 1.2196, 1.2196),\n",
    "    '1069':(2.9169, 1.2196, 1.2196),\n",
    "    '1070':(2.9169, 1.2196, 1.2196),\n",
    "    '1381':(2.9169, 1.2196, 1.2196),\n",
    "}\n",
    "\n",
    "#Total Volume\n",
    "#czi file was pyramidal, series 2 was used for measurements\n",
    "#series2 is downscaled by 3 in x and y, so volume in micron needs to be multipled by 9 to get the actual volume\n",
    "total_volume = {\n",
    "    '883': 8766163219,\n",
    "    '934': 4944596934,\n",
    "    '935': 20385745016,\n",
    "    '1066': 7342453626.988,\n",
    "    '1064': 3084277530.199,\n",
    "    '1067':3724566738.016,\n",
    "    '1069':2147206338,\n",
    "    '1070':4717521108,\n",
    "    '1381':8833829122.655,\n",
    "}\n",
    "#mutliply volume by 9 to get actual tissue volume\n",
    "total_volume = {k: v*9 for k,v in total_volume.items()}\n",
    "\n",
    "#Vessel volume and surface area calculated from the segmented vessels\n",
    "vessel_volume = {\n",
    "    '883': 2591659977.133,\n",
    "    '934': 2413540309.371,\n",
    "    '935': 5691226009.958,\n",
    "    '1066': 1252447949.720,\n",
    "    '1064': 852975560.819,\n",
    "    '1067':1267274988.222,\n",
    "    '1069':977273231.264,\n",
    "    '1070':579797002.655,\n",
    "    '1381':8793217979.968,\n",
    "}\n",
    "\n",
    "vessel_surface_area = {\n",
    "    '883': 318029422.065,\n",
    "    '934': 244531157.649,\n",
    "    '935': 968212256.941,\n",
    "    '1066': 185432718.407,\n",
    "    '1064': 151017041.746,\n",
    "    '1067': 127043494.744,\n",
    "    '1069': 75306775.307,\n",
    "    '1070': 98136863.522,\n",
    "    '1381':430553447.482,\n",
    "}\n",
    "\n",
    "merged_df = []\n",
    "\n",
    "data_list=[]\n",
    "\n",
    "#get files with extension csv and loop through them\n",
    "for file in tqdm(files):\n",
    "    fname = os.path.basename(file).split('.')[0].split(\"_\")[0]\n",
    "    pixel_size = all_px_size[fname]\n",
    "    \n",
    "    print(f\"Processing {fname}\")\n",
    "    #get fname \n",
    "    fname = os.path.basename(file).split('_')[0]\n",
    "    df = pd.read_csv(file,sep=\"\\t\") \n",
    "    df = df.iloc[2:]\n",
    "    if fname in ['883','934','935','1064','1066']:\n",
    "        treatment='IV'\n",
    "    elif fname in ['1067','1069','1070','1381']:\n",
    "        treatment='MFP'\n",
    "    else:\n",
    "        raise Exception(f\"File {fname} not found\")\n",
    "    #For surface area measurments, there could be nan values in the columns.\n",
    "    #these means that met does not touch a vessel, so change to zero\n",
    "    #if column total_touching_surface_area/2, MetNum_that_touches, number_of_touchPoints, total_touching_volume or total_touching_surface_area/2  have nan, convert it to zero\n",
    "    df[['total_touching_surface_area/2','MetNum_that_touches','number_of_touchPoints','total_touching_volume']] = \\\n",
    "        df[['total_touching_surface_area/2','MetNum_that_touches','number_of_touchPoints','total_touching_volume']].fillna(0)\n",
    "    \n",
    "    df= df.dropna(how='any')#(how='all')\n",
    "    df = df.astype(float)\n",
    "\n",
    "    #correct column names to remove leading spaces\n",
    "    df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "    df = apply_anisotropy(df,all_px_size,fname,'centroid_z')\n",
    "\n",
    "    #Keep only rows with volume corrected > 9000\n",
    "    df = df[df['Volume corrected'] > 9000]\n",
    "    df['id']=fname\n",
    "    \n",
    "    #Create a column with clone_type\n",
    "    df['clone_type'] = df.apply(calc_clone_type, axis=1)\n",
    "    \n",
    "    #remove rows with clone_type = ''\n",
    "    df = df[df['clone_type'] != '']\n",
    "\n",
    "    df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "    df['treatment'] = treatment\n",
    "    df['total_lung_volume'] = total_volume[fname]\n",
    "    df['vessel_volume'] = vessel_volume[fname]\n",
    "    df['vessel_surface_area'] = vessel_surface_area[fname]\n",
    "    \n",
    "    #Calculate major and minor axes\n",
    "    #Create empty pandas dataframe \n",
    "    major_minor_axis_df = pd.DataFrame()\n",
    "    #get major and minor axis given bounding boxes in x,y,z in columns Box.X.Min, Box.Y.Min, Box.Z.Min, Box.X.Max, Box.Y.Max, Box.Z.Max\n",
    "    major_minor_axis_df['x_axis'] = df.apply(lambda x: x['Box.X.Max'] - x['Box.X.Min'], axis=1)\n",
    "    major_minor_axis_df['y_axis'] = df.apply(lambda x: x['Box.Y.Max'] - x['Box.Y.Min'], axis=1)\n",
    "    major_minor_axis_df['z_axis'] = df.apply(lambda x: x['Box.Z.Max'] - x['Box.Z.Min'], axis=1)\n",
    "    #apply anisotropy correction in z axis\n",
    "    major_minor_axis_df = apply_anisotropy(major_minor_axis_df,all_px_size,fname,'z_axis')\n",
    "    df['minor_axis'] = major_minor_axis_df.min(axis=1)\n",
    "    df['major_axis'] = major_minor_axis_df.max(axis=1)\n",
    "    df['elongation'] = df['major_axis']/df['minor_axis']\n",
    "    merged_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IV' 'MFP']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "treatment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "1064",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1066",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1067",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1069",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1070",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1381",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "883",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "934",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "935",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e105192e-373b-42e1-982d-4c0b2be265e7",
       "rows": [
        [
         "IV",
         "2896.0",
         "2356.0",
         null,
         null,
         null,
         null,
         "2781.0",
         "405.0",
         "5356.0"
        ],
        [
         "MFP",
         null,
         null,
         "1380.0",
         "1426.0",
         "3406.0",
         "1146.0",
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>1064</th>\n",
       "      <th>1066</th>\n",
       "      <th>1067</th>\n",
       "      <th>1069</th>\n",
       "      <th>1070</th>\n",
       "      <th>1381</th>\n",
       "      <th>883</th>\n",
       "      <th>934</th>\n",
       "      <th>935</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IV</th>\n",
       "      <td>2896.0</td>\n",
       "      <td>2356.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2781.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>5356.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MFP</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1380.0</td>\n",
       "      <td>1426.0</td>\n",
       "      <td>3406.0</td>\n",
       "      <td>1146.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "id           1064    1066    1067    1069    1070    1381     883    934  \\\n",
       "treatment                                                                  \n",
       "IV         2896.0  2356.0     NaN     NaN     NaN     NaN  2781.0  405.0   \n",
       "MFP           NaN     NaN  1380.0  1426.0  3406.0  1146.0     NaN    NaN   \n",
       "\n",
       "id            935  \n",
       "treatment          \n",
       "IV         5356.0  \n",
       "MFP           NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate all dataframes\n",
    "merged_all = pd.concat(merged_df)\n",
    "print(merged_all.treatment.unique())\n",
    "#Verify the number of unique ids\n",
    "merged_all.groupby(['treatment','id']).size().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete column Volume as we use Volume corrected\n",
    "merged_all = merged_all.drop(columns=['Volume'])\n",
    "#if underscore in row clone_type, create new column and give value polyclonal, otherwise monoclonal\n",
    "merged_all['met_type_cat'] = np.where(merged_all['clone_type'].str.contains('_'), 'polyclonal', 'monoclonal')\n",
    "merged_all.to_csv(\"./MFP_IV_combined_raw_data.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
